**Postcards from my Jungle**

The parameters of this assignment were a little difficult to navigate, and the fact that we were hit with a snowstorm on a class day did not make it any easier. Regardless, I still took the time to study the provided notebooks and gain a better understanding of the process. After hours of trials and error and spending more time than I probably should have, I managed to grasp some basic concepts and achieve some results.

The most challenging aspect came to the image generation. My primary strategy for training the model to its best potential centered around the dataset. After browsing through animal photos on Hugging Face, I selected one of the birds. I figured this would work well since birds have recurring features, which the model might find easier to recognize. My second strategy involved increasing the sample size of the images, and raising the number of epochs to about 150\. Pushing it further would cause the GPU to run out of memory. In fact, I ended up having to pay for additional computing units after exhausting my resources. Though I tried using Kaggle as a free alternative, I found the lack of an AI assistant to be a drawback.

Training my model and generating images took approximately three hours. Similar to the example, my results are somewhat abstract and difficult to interpret. However, the recurring painterly greenness in the outputs somewhat adhered to the whole jungle theme, so at least there was that. Apart from image generation, everything else was pretty simple. Since I was working with birds, primarily owls and puffins, I used my Stable Audio model to generate a mix of hoots, tweets, screeches, and squawks. Then, I prompted my GPT model to generate two to three syllable words based on these sounds.

With more time and patience, I believe that the results could be significantly improved. Finding the right dataset is crucial. As we saw with the butterfly generation model, the fact that each butterfly image was still with no background allowed the model to recognize patterns more effectively. Unfortunately, in most datasets, elements are usually all over the place, making pattern recognition more challenging. If we could guide the model with prompts to give it an idea on what itâ€™s meant to look for, we could enhance the training process. Lastly, with greater computing power, we could further increase the sample size and the number of training epochs, leading to much better results.

